# AmbedkarGPT-Intern-Task

This is a small local RAG (Retrieval-Augmented Generation) project built as part of an internship task.  
The idea is simple: take a short speech by Dr. B. R. Ambedkar, store it as searchable embeddings, and allow users to ask questions about the text â€” with answers generated by a local language model, not the internet.

Everything runs offline using:

- LangChain
- Chroma DB
- HuggingFace Embeddings
- Ollama (Mistral 7B)

---

## Functions

- Loads text from `speech.txt`
- Splits the text into small overlapping chunks
- Creates embeddings using `all-MiniLM-L6-v2`
- Stores these embeddings inside a local Chroma database
- Retrieves relevant text when a question is asked
- Sends the retrieved context + question to a local LLM (Mistral)
- Returns an answer **strictly based on the provided content**

So this is a minimal working example of a **local RAG pipeline**.

---

## Requirements

- Python **3.8+**
- **Ollama installed**
- `mistral` model available locally:



